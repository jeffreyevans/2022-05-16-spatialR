---
title: "Graph-theoretic and Gravity Models"
author: "Melanie A. Murphy & Jeffrey S. Evans"
date: "2/21/2022"
output:
  html_document: 
    toc: yes
    keep_md: yes
    code_folding: hide
---

This is an introduction to graph theoretic and gravity modeling approaches for quantifying geneflow in spatial systems. 

# Setup

#### Add required libraries and set working environment


```{r, class.source = "fold-show", eval=FALSE}

p <- c("raster", "rgdal", "igraph", "sp", "GeNetIt", "spatialEco", 
       "sf", "terra", "sfnetworks", "spdep", "dplyr", "tmap", "devtools") 
  if(any(!unlist(lapply(p, requireNamespace, quietly=TRUE)))) { 
    m = which(!unlist(lapply(p, requireNamespace, quietly=TRUE)))
	  suppressMessages(invisible(lapply(p[-m], require,    
	                   character.only=TRUE)))
    stop("Missing library, please install ", paste(p[m], collapse = " "))
  } else {
    if(packageVersion("GeNetIt") < "0.1-5") {
      remotes::install_github("jeffreyevans/GeNetIt")
	} 
    suppressMessages(invisible(lapply(p, require, character.only=TRUE)))
  }

# set your working directory here
setwd(wd <- "C:/evans/GITS/Gravity")

# indicates UTM 11 NAD83 projection
prj = 32611 

# Some needed functions
back.transform <- function(y) exp(y + 0.5 * stats::var(y))
rmse <- function(p, o){ sqrt(mean((p - o)^2)) }

# Download data
d = "https://spatialr.s3.us-west-2.amazonaws.com/Gravity/data.zip"
download.file(d, destfile="data.zip", mode="wb")
  unzip("data.zip")
    file.remove("data.zip")

```


# Wetland complex data preparation 

In this sections, we will read in all wetland locations in the study area and calculate a few graph-based metrics to assign to wetland sites that data was collected at. This allows us to put our samples into the context of the larger wetland system thus, accounting for proximity and juxtaposition. 

## Read in wetlands data 

Read file "Wetlands.csv" and make a spatial object.

  Hints: read.csv, coordinates, st_as_sf

```{r, eval=FALSE}

wetlands <- read.csv(file.path(wd, "data", "Wetlands.csv"), 
                    header = TRUE)
  head(wetlands)
  
  
wetlands <- st_as_sf(wetlands, coords = c("X", "Y"), 
                     crs = 32611, agr = "constant") 

```

## Create wetlands graph 

Create Gabriel graph from the wetlands to represent a "realization" of connectivity and spatial arrangement. 

|    Hints: gabrielneigh, graph2nb, nb2lines  

What other types of graphs could you build? What wetlands are connected to each other based on the graph?


```{r, eval=FALSE}

# Derive Gabriel graph
gg <- graph2nb(gabrielneigh(st_coordinates(wetlands)),sym=TRUE)
  plot(gg, coords=st_coordinates(wetlands))

# Coerce to sf line object (will be used to create igraph object)
gg <- nb2lines(gg, coords = sf::st_coordinates(wetlands), 
	             proj4string = prj, as_sf=TRUE)

    
```

## Graph metrics

Using the sf line/graph we just made coerce to sfnetworks, then calculate graph metrics of betweenness and closeness with weights and degree.   
   
|    Hints: as_sfnetwork, activate, degree, betweenness, closeness

Do you think this graph is ecologically meaningful? What information, in the graph structure itself, might this graph contain?

degree - the number of connections a node has Calculate 
betweenness - the number of shortest paths going through a node
closensess - 

```{r, eval=FALSE}

# Create sfnetwork, igraph object
wg <- as_sfnetwork(gg, edges=gg, nodes=wetlands, directed = FALSE,
                  node_key = "SiteName", length_as_weight = TRUE, 
				          edges_as_lines = TRUE)

# Calculate weights and graph metrics
w <- wg %>% activate("edges") %>% pull(weight) %>% as.numeric()
  w[w <= 0] <- 1
    w = w / sum(w)

# calculate and add betweenness, degree and  closeness     
wetlands$betweenness <- igraph::betweenness(wg, directed=FALSE, weights=w)
wetlands$degree <- igraph::degree(wg)
wetlands$closeness <- igraph::closeness(wg, weights=w)
 	
```

##  Plot graph metric 

Plot results using the graph edges and wetlands points with the attribute "betweenness"

|    Hint: plot, st_geometry, add=TRUE

```{r, eval=FALSE}

plot(st_geometry(gg), col="grey")
  plot(wetlands["betweenness"], pch=19,  
       cex=0.75, add=TRUE)
     box()
	 title("Wetlands Gabriel graph betweenness")

```


# Wetland field-data preparation 

In this section we will read the field data, add the node metrics we just calculated.

## Read site data

#### Using RALU_Site.csv, read in the data, add the node data (betweenness and degree), create a spatial object that includes the node data. Look at the RALU_Site file.  What are the fields here? What data are included?  

|    Hints: which, merge

Using names is dangerous as, small changes in names can result in  non-matches. In this case, the ID fields are not consistent (data were collected at different times for different purposes originally). However, names are standardized in a drop-down list of a database. So they are a matching field. My preference is do to this type of  operation on a numeric field.


```{r, eval=FALSE}

sites <- read.csv(file.path(wd, "data", "RALU_Site.csv"), 
                  header = TRUE)
  sites$SiteID <- as.character(sites$SiteID)

nodestats <- st_drop_geometry(wetlands[,c(3,5:7)])
  nodestats <- nodestats[which(nodestats$SiteName %in% sites$SiteName),] 
sites <- merge(nodestats, sites, by="SiteName")

sites <- st_as_sf(sites, coords = c("X", "Y"), 
                 crs = prj, agr = "constant") 

```


## Saturated Graph

## Create graph from site locations 

|    Hints: knn.graph, make sure to use correct field 

To assess connectivity using a gravity model, we need to build a graph from the occupied frog sites create a graph. This could be any type of graph, but I generally use saturated or pruned by some maximum distance.

```{r, eval=FALSE}

dist.graph <- knn.graph(sites, row.names = sites$SiteName)
  dist.graph <- merge(dist.graph, st_drop_geometry(sites), 
                      by.y="SiteName", by.x="from_ID")
    dist.graph <- dist.graph[,-c(11:19)] # drop extra columns
    
## Can create a distance-constrained graph with max.dist arg (not run)
# dist.graph <- knn.graph(sites, row.names = sites$SiteName, max.dist=5000)

```


##  Merge the graph with genetic distance. 

This involves: reading in RALU_Dps.csv genetic distance, convert to flow (1-distance), unfold matrix into a dataframe then, merge graph with genetic distances 

|    Hints: dmatrix.df

Note; if you get stuck on this step, read in gdist.csv


```{r, eval=FALSE}

gdist <- read.csv(file.path(wd, "data", "RALU_Dps.csv"), header=TRUE) 
  rownames(gdist) <- t(names(gdist))
	  
# Make a matrix, gdist, unfold data  
gdist <- dmatrix.df(as.matrix(gdist)) 
  names(gdist) <- c("FROM", "TO", "GDIST") #unfold the file
    gdist <- gdist[!gdist$FROM == gdist$TO ,]
    gdist[,1] <-sub("X", "", gdist[,1])
    gdist[,2] <-sub("X", "", gdist[,2])
    gdist <- cbind(from.to=paste(gdist[,1], gdist[,2], sep="."), 
                   gdist)

# Transform genetic distance to genetic flow
gdist$GDIST <- flow(gdist$GDIST)

# Merge data  
dist.graph$from.to <- paste(dist.graph$i, dist.graph$j, sep=".")
  dist.graph <- merge(dist.graph, gdist, by = "from.to")

```

# Spatial model data prepration

##  Read raster data using terra

|    Hint: list.files, rast

Note; R uses regular expressions so, a wildcard is not "*"


```{r, eval=FALSE}

list.files(file.path(wd,"data"), "tif$")
xvars <- rast(list.files(file.path(wd,"data"), "tif$", 
                        full.names = TRUE))

```


##  Reclassify wetlands 

Reclassify NLCD, in xvars, into a single class

   Hint: matrix, reclassify 


```{r, eval=FALSE}

m <- c(0,10.8, 0,10.9,12.1,1,12.9,89.5,0, 89.1,95.1,1,95.9,100,0 )
  reclass <- matrix(m, ncol=3, byrow=TRUE)
  
wetlnd <- classify(xvars[["nlcd"]], reclass)
  names(wetlnd) <- "wetlnd"
    xvars <- c(xvars, wetlnd)
    
```


##  Calculate the proportion of the landscape around sites 

Assign proportion of landcover that is wetland to sites as pwetland

|    Hint: write function, extract

You want to know if areas of dense wetlands produce more frogs? What buffer distance will you use?

```{r, eval=FALSE}

## method 1 (can result in Inf if all zero)
#  prop.land <- function(x) {
#   length(x[x==1]) / length(x)  
#  }

## method 2 (no divide by zero error)
prop.land <- function(x) {
  prop.table(table(factor(x, levels=c(0,1))))[2]
}

b <- st_buffer(sites, 300)
pwetland <- extract(wetlnd, vect(b))
  pwetland <- tapply(pwetland[,2], pwetland[,1], prop.land)
  
# Add the % wetland back to the dataframe
sites$pwetland <- as.numeric(pwetland)

```

##  Add values of rasters to sample sites 

This adds potential "at site" variables, keep as sf POINT class object
 
|    Hint: st_sf, terra, extract

```{r, eval=FALSE}

  stats <- extract(xvars[[-6]], vect(sites))
  sites <- st_sf(data.frame(as.data.frame(sites), stats), 
                 geometry=sites$geometry)

```


##  Add raster covariates to graph edges (lines). 

Remove nlcd and wetlnd rasters before calculating statistics  
 
|    Hint: graph.statistics


```{r, eval=FALSE}

idx <- which(names(xvars) %in% c("nlcd","wetlnd"))
suppressWarnings(
  stats <- graph.statistics(dist.graph, r = xvars[[-idx]], 
                            buffer= NULL, stats = c("min",         
                            "mean","max", "var", "median")) )

  dist.graph <- st_sf(data.frame(as.data.frame(dist.graph), 
                      stats), geometry=dist.graph$geometry)

```

##  What about categorical variables? 

Moments are nonsensical. Create a function for returning the % wetland between sites. Then use it to calculate an additional statistic and, add result to the graph.  
 
|    Hint: ifelse, table, prop.table, factor, graph.statistics
 
Are there other categorical variables that you think may be ecologically important?


```{r, eval=FALSE}

wet.pct <- function(x) {
  x <- ifelse(x == 11 | x == 12 | x == 90 | x == 95, 1, 0)
    prop.table(table(factor(x, levels=c(0,1))))[2] 
}   

suppressWarnings(
  wetstats <- graph.statistics(dist.graph, r=xvars[["nlcd"]], 
                               buffer= NULL, stats = c("wet.pct")) )
dist.graph$wet.pct.nlcd <- as.numeric(wetstats[,1]) 

```

 ##  Evaluate node and edge correlations
 
We need to evaluate correlations in the data to not overdispersion our models. Note, we are not going to actually remove the correlated variables but, just go through a few methods of evaluating them. The code to remove colinear variables is commented out for reference. We do have to log transform the data as to evaluate the actual model structure.   

Hint: cor, collinear
 
```{r, eval=FALSE}

node.var <- c("degree", "betweenness", "Elev", "Length", "Area", "Perim", 
              "Depth", "pH","Dforest","Drock", "Dshrub", "pwetland", "cti",
			        "dd5", "ffp","gsp","pratio","hli","rough27","srr")

p = 0.8 
s <- st_drop_geometry(sites)[,node.var]
  for(i in 1:ncol(s)) {
    s[,i] <- ifelse(s[,i] <= 0, 0.00001, log(s[,i]))
  }
  
#### site correlations  
site.cor <- cor(s, y = NULL, 
                use = "complete.obs", 
                method = "pearson")
	diag(site.cor) <- 0			  		
cor.idx <- which(site.cor > p | site.cor < -p, arr.ind = TRUE)
  cor.names <- vector()
  cor.p <- vector()
    for(i in 1:nrow(cor.idx)) {
	  cor.p[i] <- site.cor[cor.idx[i,][1], cor.idx[i,][2]]
      cor.names [i] <- paste(rownames(site.cor)[cor.idx[i,][1]],
                       colnames(site.cor)[cor.idx[i,][2]], sep="_")
	}	
data.frame(parm=cor.names, p=cor.p)

( node.cor <- collinear(s, p=p) )

# node.var <- node.var[-which(node.var %in% node.cor)]

```

 ##  Add node data 
 
Build and add node (at site) level data to graph then merge edge (distance graph) and edge (site) data.   

   Hint: build.node.data, merge


```{r, eval=FALSE}

node.var <- c("degree", "betweenness", "Elev", "Length", "Area", "Perim", 
              "Depth", "pH","Dforest","Drock", "Dshrub", "pwetland", "cti",
			  "dd5", "ffp","gsp","pratio","hli","rough27","srr")  

node <- build.node.data(st_drop_geometry(sites), group.ids = "SiteID", 
                        from.parms = node.var)

```

# Merge node and edges for model data.frame

```{r, eval=FALSE}

gdata <- merge(st_drop_geometry(dist.graph)[c(1,2,5,11,14,7)], node, 
               by.x="SiteID", by.y="SiteID")
	gdata <- merge(gdata, st_drop_geometry(dist.graph)[c(11, 8:10, 15:55)], 
	               by.x="SiteID", by.y="SiteID") 
    # log transform matrix
    for(i in 5:ncol(gdata)) {
      gdata[,i] <- ifelse(gdata[,i] <= 0, 0.00001, log(gdata[,i]))
    }

```

# Gravity model


## Develop hypothesis   

Think about type of hypothesis that you want to test and What types of constraint are needed? Write out model statements that group parameters into hypothesis and run models. Remember to run a NULL that is just distance. 


```{r, eval=FALSE}

# null model (under Maximum Likelihood) 
( null <- gravity(y = "GDIST", x = c("length"), d = "length", group = "from_ID", 
                  data = gdata, fit.method = "ML", ln = FALSE) )

# Fish hypothesis (under Maximum Likelihood) 
( depth <- gravity(y = "GDIST", x = c("length","from.Depth"), d = "length", 
                   group = "from_ID", data = gdata, fit.method = "ML", ln = FALSE) )

# Productivity hypothesis (under Maximum Likelihood) 
( product <- gravity(y = "GDIST", x = c("length", "from.ffp", "from.hli"), 
                     d = "length",  group = "from_ID", data = gdata, 
					 fit.method = "ML", ln = FALSE) )

# Climate hypothesis (under Maximum Likelihood) 
( climate <- gravity(y = "GDIST", x = c("length", "from.ffp", "from.pratio"), 
                     d = "length", group = "from_ID", data = gdata, 
					 fit.method = "ML",  ln = FALSE) )

# Wetlands hypothesis (under Maximum Likelihood) 
( wetlands <- gravity(y = "GDIST", x = c("length", "from.degree", "from.betweenness", "from.pwetland"), 
                      d = "length", group = "from_ID", data = gdata, fit.method = "ML",
					  ln = FALSE) )

# Topography hypothesis (under Maximum Likelihood) 
( topo <- gravity(y = "GDIST", x = c("length", "median.srr", "median.rough27"), d = "length", 
                  group = "from_ID", data = gdata, fit.method = "ML",
				  ln = FALSE) )

# Habitat hypothesis (under Maximum Likelihood) 
( habitat <- gravity(y = "GDIST", x = c("length", "wet.pct.nlcd", "median.gsp"), 
                     d = "length", group = "from_ID", data = gdata, fit.method = "ML",
					 ln = FALSE, method="ML") )

# Global model (under Maximum Likelihood) 
( global <- gravity(y = "GDIST", x = c("length", "wet.pct.nlcd", "median.gsp", 
                    "from.Depth", "from.ffp", "from.hli", "from.pratio", "from.degree", 
					"from.betweenness", "from.pwetland", "median.srr", "median.rough27"), 
					d = "length", group = "from_ID", data = gdata, fit.method = "ML",
					ln = FALSE) )


```

##  Compare competing models. 

Should you use ML or REML? Create diagnostic plots

|    Hint: compare.models 

Can you directly compare ML and REML? Why not?

```{r, eval=FALSE}

compare.models(null, depth, product, climate, wetlands, 
               topo, habitat, global)

compare.models(depth, product, climate, wetlands, topo, 
               habitat, global)

par(mfrow=c(2,3))
   for (i in 1:6) { plot(null, type=i) } 

```


##  Fit final model(s)

THen compair them and calculate effect size

|    Hint: compare.models, gravity,es


```{r, eval=FALSE}

# Habitat fit (under REML)
h <- c("length", "wet.pct.nlcd", "median.gsp")
habitat_fit <- gravity(y = "GDIST", x = h, d = "length", group = "from_ID",
                      data = gdata, ln=FALSE)

# global fit (under REML)
g <-  c("length", "wet.pct.nlcd", "median.gsp", "from.Depth", "from.ffp",
        "from.hli", "from.pratio",  "from.degree", "from.betweenness",  
        "from.pwetland", "median.srr",  "median.rough27")
global_fit <- gravity(y = "GDIST", x = g, d = "length", 
                      group = "from_ID", data = gdata, ln=FALSE)


gravity.es(habitat_fit)
gravity.es(global_fit)

par(mfrow=c(2,3))
   for (i in 1:6) { plot(global_fit, type=i) } 
dev.new()
par(mfrow=c(2,3))
   for (i in 1:6) { plot(habitat_fit, type=i) } 

```

##  Back predict global_fit model

|    Hint: predict

```{r, eval=FALSE}

gd <- back.transform(gdata$GDIST)

# Make individual-level (group) predictions (per slope) and
# show RMSE 
global.p <- predict(global_fit, y = "GDIST", x = g,  
                    newdata=gdata, groups = gdata$from_ID,
				    back.transform = "simple")
habitat.p <- predict(habitat_fit, y = "GDIST", x = h,  
                     newdata=gdata, groups = gdata$from_ID,
			         back.transform = "simple")

cat("RMSE of global", rmse(global.p, gd), "\n")
cat("RMSE of habitat", rmse(habitat.p, gd), "\n")

```

##  Aggregrate estimates and plot 

We can aggregrate estimates back to the edges and nodes. An intercative map can be created using the tmap package 

|    Hint: tapply

```{r, eval=FALSE}

# Aggregate estimates to graph and node
global.p <- data.frame(EID = gdata$from.to,
                       NID = gdata$from_ID,  
                       p=global.p)

edge.p <- tapply(global.p$p, global.p$EID, mean)
  dist.graph$global.flow <- edge.p
  
node.p <- tapply(global.p$p, global.p$NID, mean)
node.var <- tapply(global.p$p, global.p$NID, var)
idx <- which(sites$SiteName %in% names(node.p))
  sites$global.flow[idx] <- node.p
  sites$global.var[idx] <- node.var

# Plot results using tmap
pal <- colorRampPalette(rev(c("red","orange","blue")), bias=0.15)
tmap_mode(c("plot", "view")[2])
  tm_shape(dist.graph) +
     tm_lines("global.flow", palette=pal(10)) +
       tm_shape(sites) +
  	     tm_symbols(col = "global.flow", 
		            size = "global.var", 
	                shape = 20, scale = 0.75, palette=pal(10))

```